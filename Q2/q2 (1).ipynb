{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4cc3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import ArxivAPIWrapper\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import fitz\n",
    "import streamlit as st\n",
    "#from streamlit_chat import message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_API_KEY=\"\" #Enter your OpenAI API KEY\n",
    "class Embedder:\n",
    "    \"Embedding engine to create doc embeddings\"\n",
    "    def __init__(self,engine='OpenAI'):\n",
    "        \"\"\"\"Specify embedding model\n",
    "        Args:\n",
    "        -----------------\n",
    "        engine:the embedding model.\"\"\"\n",
    "        if engine=='OpenAI':\n",
    "            self.embeddings=OpenAIEmbeddings(api_key=OPEN_API_KEY)\n",
    "        else:\n",
    "            raise KeyError(\"Currently unsupported chat model type\")\n",
    "    def load_process(self,path):\n",
    "        \"\"\"Load and process PDF document.\n",
    "        Args:\n",
    "        ---------\n",
    "        path: path of the paper.\"\"\"\n",
    "        #load pdf\n",
    "        loader=PyMuPDFLoader(path)\n",
    "        documents=loader.load()\n",
    "        #process pdf\n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "        self.documents=text_splitter.split_documents(documents)\n",
    "    def create_vectorstore(self,store_path):\n",
    "        \"\"\"Create vector store for doc Q&A.\n",
    "        Args:\n",
    "        -------------------------\n",
    "        store_path:path of vector store.\n",
    "        Outputs:\n",
    "        vectorstore:the created vector store for holding embeddings\"\"\"\n",
    "        if not os.path.exists(store_path):\n",
    "            print('Embedding not found! Creating new ones')\n",
    "            self.vectorstore=FAISS.from_documents(self.documents,self.embeddings)\n",
    "            self.vectorstore.save_local(store_path)\n",
    "            \n",
    "        else:\n",
    "            print(\"Embeddings found! Loaded the computed ones\")\n",
    "            self.vectorstore=FAISS.load_local(store_path,self.embeddings,allow_dangerous_deserialization=True)\n",
    "        return self.vectorstore\n",
    "    def create_summary(self,llm_engine=None):\n",
    "        \"\"\"Create paper summary.\n",
    "        The summary is created by using LangChain's summarize_chain.\n",
    "        Args:\n",
    "        --------------\n",
    "        llm_engine: llm to use.\n",
    "        Outputs:\n",
    "        --------------\n",
    "        summary: the summary of the paper\"\"\"\n",
    "\n",
    "        if llm_engine is None:\n",
    "            raise KeyError('please specify a LLM engine to perform summarization')\n",
    "        elif llm_engine=='OpenAI':\n",
    "            llm=ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0.8,api_key=OPEN_API_KEY)\n",
    "        else:\n",
    "            raise KeyError('Currently unsupported chat model type')\n",
    "            \n",
    "        #Use LLM to summarize the paper\n",
    "        chain=load_summarize_chain(llm,chain_type='stuff')\n",
    "        summary=chain.run(self.documents[:20])\n",
    "        return summary\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot(ABC):\n",
    "    \"\"\"Class definition for a single chatbot with memory,created with LangChain. \"\"\"\n",
    "    def __init__(self,engine):\n",
    "        \"\"\"Initialize the LLM and its associated memory.\n",
    "        The memory can be an LangChain memory object,or a list of chat history.\n",
    "        Args:\n",
    "        --------------------\n",
    "        engine: llm to be used.\"\"\"\n",
    "        #Initialize llm\n",
    "        if engine=='OpenAI':\n",
    "            self.llm=ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0.8,api_key=OPEN_API_KEY)\n",
    "        else:\n",
    "            raise KeyError(\"Currently unsupported chat model type\")\n",
    "    @abstractmethod\n",
    "    def instruct(self):\n",
    "        \"\"\"Determine the context of chatbot interaction\"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        \"\"\"Action produced by the chatbot.\"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Prompt engineering for chatbot.\"\"\"\n",
    "        pass\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811d1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JournalistBot(Chatbot):\n",
    "    \"\"\"Class definition for the journalist bot,created with Langchain.\"\"\"\n",
    "    def __init__(self,engine):\n",
    "        \"\"\"Setup journalist bot.\n",
    "        Args:\n",
    "        -------------------------\n",
    "        engine: llm model to be used.\"\"\"\n",
    "        #Instantiate llm\n",
    "        super().__init__(engine)\n",
    "        #Instantiate memory\n",
    "        self.memory=ConversationBufferMemory(return_messages=True)\n",
    "    def instruct(self,topic,abstract):\n",
    "        \"\"\"Determine the context of journalist chatbot.\n",
    "        Args:\n",
    "        -----------\n",
    "        topic: the topics of the paper\n",
    "        abstract: the abstract of the paper.\"\"\"\n",
    "        self.topic=topic\n",
    "        self.abstract=abstract\n",
    "        #Define prompt template\n",
    "        prompt=ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"\"\"{input}\"\"\")\n",
    "            \n",
    "        ])\n",
    "        #Create conversation chain\n",
    "        self.conversation=ConversationChain(memory=self.memory,prompt=prompt,llm=self.llm,verbose=False)\n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Specify the behaviour of the journalist chatbot.\n",
    "        The prompt is generated and optimized with GPT-4.\n",
    "        outputs:\n",
    "        ----------------\n",
    "        prompts: instructions for chatbot.\"\"\"\n",
    "        prompt=f\"\"\"You are a technical journalist interested in {self.topic}, \n",
    "            Your task is to distill a recently published scientific paper on this topic through\n",
    "            an interview with the author, which is played by another chatbot.\n",
    "            Your objective is to ask comprehensive and technical questions \n",
    "            so that anyone who reads the interview can understand the paper's main ideas and contributions, \n",
    "            even without reading the paper itself. \n",
    "            You're provided with the paper's summary to guide your initial questions.\n",
    "            You must keep the following guidelines in mind:\n",
    "            - Focus exclusive on the technical content of the paper.\n",
    "            - Avoid general questions about {self.topic}, focusing instead on specifics related to the paper.\n",
    "            - Only ask one question at a time.\n",
    "            - Feel free to ask about the study's purpose, methods, results, and significance, \n",
    "            and clarify any technical terms or complex concepts. \n",
    "            - Your goal is to lead the conversation towards a clear and engaging summary.\n",
    "            - Do not include any prefixed labels like \"Interviewer:\" or \"Question:\" in your question.\n",
    "    \n",
    "            [Abstract]: {self.abstract}\"\"\"\n",
    "        return prompt\n",
    "    def step(self,prompt):\n",
    "        \"\"\"Journalist chatbot asks question\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "        prompt: Previous answer provided by the author bot.\"\"\"\n",
    "        response=self.conversation.predict(input=prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b91d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorBot(Chatbot):\n",
    "    \"\"\"Class definition for the author bot,created with Langchain.\"\"\"\n",
    "    def __init__(self,engine,vectorstore,debug=False):\n",
    "        \"\"\"Select the llm, as well as instantiate the memory for creating language chain in langchain.\n",
    "        \n",
    "        Args:\n",
    "        ----------\n",
    "        engine: llm model to be used.\n",
    "        vectorstore: embedding vectors of the pdf.\"\"\"\n",
    "        #Instantiate llm\n",
    "        super().__init__(engine)\n",
    "        #Intantiate memory\n",
    "        self.chat_history=[]\n",
    "        #Instantiate embedding index\n",
    "        self.vectorstore=vectorstore\n",
    "        self.debug=debug\n",
    "    def instruct(self,topic):\n",
    "        \"\"\"Determine the context of author chatbot.\n",
    "        Args:\n",
    "        -----------\n",
    "        topic: the topic of the paper.\"\"\"\n",
    "        #specify topic\n",
    "        self.topic=topic\n",
    "        #define prompt template\n",
    "        qa_prompt=ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "        ])\n",
    "        # Create conversation chain\n",
    "        self.conversation_qa=ConversationalRetrievalChain.from_llm(llm=self.llm,verbose=self.debug,retriever=self.vectorstore.as_retriever(search_kwargs={'k':5}),return_source_documents=True,combine_docs_chain_kwargs={'prompt':qa_prompt})\n",
    "    def step(self,prompt):\n",
    "        \"\"\"Author chatbot answers question.\n",
    "        Args:\n",
    "        ----------\n",
    "        prompt: question raised by journalist bot.\n",
    "        \n",
    "        Outputs:\n",
    "        ------------\n",
    "        answer: the author bot's answer\n",
    "        source_documents: documents that author bot used to answer questions\"\"\"\n",
    "        response=self.conversation_qa({'question':prompt,'chat_history':self.chat_history})\n",
    "        self.chat_history.append((prompt,response['answer']))\n",
    "        return response['answer'],response['source_documents']\n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Specify the behaviour of the author chatbot.\n",
    "        \n",
    "        outputs:\n",
    "        ------------\n",
    "        prompt: instructions for the chatbot.\"\"\"\n",
    "        prompt = f\"\"\"You are the author of a recently published scientific paper on {self.topic}.\n",
    "                You are being interviewed by a technical journalist who is played by another chatbot and\n",
    "                looking to write an article to summarize your paper.\n",
    "                Your task is to provide comprehensive, clear, and accurate answers to the journalist's questions.\n",
    "                Please keep the following guidelines in mind:\n",
    "                - Try to explain complex concepts and technical terms in an understandable way, without sacrificing accuracy.\n",
    "                - Your responses should primarily come from the relevant content of this paper, \n",
    "                  which will be provided to you in the following, but you can also use your broad knowledge in {self.topic} to \n",
    "                  provide context or clarify complex topics. \n",
    "                - Remember to differentiate when you are providing information directly from the paper versus \n",
    "                  when you're giving additional context or interpretation. Use phrases like 'According to the paper...' for direct information, \n",
    "                  and 'Based on general knowledge in the field...' when you're providing additional context.\n",
    "                - Only answer one question at a time. Ensure that each answer is complete before moving on to the next question.\n",
    "                - Do not include any prefixed labels like \"Author:\", \"Interviewee:\", Respond:\", or \"Answer:\" in your answer.\n",
    "    \"\"\"\n",
    "    \n",
    "        prompt += \"\"\"Given the following context, please answer the question.\n",
    "    \n",
    "    {context}\"\"\"\n",
    "    \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835a4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_PDF(file_path,phrases,output_path):\n",
    "    \"\"\"Search and highlight given texts in PDF.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "    file_path: PDF file path\n",
    "    phrases: a list of texts(in strings)\n",
    "    output_path: save and output PDF\"\"\"\n",
    "    #Open PDF\n",
    "    doc=fitz.open(file_path)\n",
    "    #Search doc\n",
    "    for page in doc:\n",
    "        for phrase in phrases:\n",
    "            text_instances=page.search_for(phrase)\n",
    "            #Highlight\n",
    "            for inst in text_instances:\n",
    "                highlight=page.add_highlight_annot(inst)\n",
    "    doc.save(output_path,garbage=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55742596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings found! Loaded the computed ones\n",
      "üë®‚Äçüè´ Journalist: What motivated you to explore the application of transformers to image recognition tasks in your paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"?\n",
      "üë©‚Äçüéì Author: The motivation behind exploring the application of transformers to image recognition tasks in our paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" was to investigate whether transformers, which have been highly successful in natural language processing (NLP), could also excel in computer vision. While transformers had become the standard for NLP tasks, their applications in computer vision were limited, with attention usually used alongside convolutional networks or to replace certain components of CNNs. We aimed to challenge the notion that transformers rely on CNNs in vision tasks and demonstrate that a pure transformer applied directly to sequences of image patches could achieve competitive performance in image classification tasks. By exploring the potential of transformers in image recognition, we aimed to broaden the scope of transformer applications beyond NLP.\n",
      "For details, please check the highlighted text on page(s): 9, 1, 2\n",
      "Do you want to ask any question(y/n)y\n",
      "Write your questionWhy is the paper named an image is worth 16x16 words and how does it differ from cnns\n",
      "üë®‚Äçüè´ User: Why is the paper named an image is worth 16x16 words and how does it differ from cnns\n",
      "üë©‚Äçüéì Author: The name of the paper, \"An Image is Worth 16x16 Words,\" reflects the approach taken in image recognition tasks using transformers. In this paper, the authors introduce the Vision Transformer (ViT) model, which processes images as sequences of image patches, similar to how tokens are processed in natural language processing tasks. By breaking down the image into smaller patches and treating them as individual tokens, the ViT model directly applies the transformer architecture to image data.\n",
      "\n",
      "What sets this approach apart from convolutional neural networks (CNNs) is that traditionally, in computer vision tasks, attention mechanisms like those in transformers were either used alongside CNNs or to replace certain components of CNNs. However, the ViT model demonstrates that it is not necessary to rely on CNNs for image recognition tasks. By directly applying transformers to image patches, the ViT model challenges the conventional wisdom that CNNs are essential for computer vision tasks. This approach showcases that transformers can achieve competitive performance on image classification tasks without the need for convolutional layers.\n",
      "For details, please check the highlighted text on page(s): 1, 2\n",
      "Do you want to ask any question(y/n)n\n",
      "üë®‚Äçüè´ Journalist: How does the design of the Vision Transformer (ViT) model leverage the success of transformers in natural language processing to achieve competitive performance in image recognition tasks without relying on convolutional neural networks (CNNs)?\n",
      "üë©‚Äçüéì Author: The design of the Vision Transformer (ViT) model leverages the success of transformers in natural language processing by applying the self-attention mechanism to process sequences of image patches directly. This approach eliminates the need to rely on convolutional neural networks (CNNs) for image recognition tasks. By utilizing a standard Transformer encoder typically used in NLP, ViT can effectively capture dependencies between different image patches and learn meaningful representations from the input data. This strategy allows ViT to achieve competitive performance in image recognition tasks, even surpassing state-of-the-art convolutional networks, while requiring fewer computational resources for training.\n",
      "For details, please check the highlighted text on page(s): 4, 9, 1, 2\n",
      "Do you want to ask any question(y/n)n\n",
      "üë®‚Äçüè´ Journalist: How does the ViT model process sequences of image patches using the self-attention mechanism to capture dependencies between different patches and learn meaningful representations, ultimately achieving competitive performance in image recognition tasks without the need for convolutional neural networks (CNNs)?\n",
      "üë©‚Äçüéì Author: The Vision Transformer (ViT) model processes sequences of image patches by first extracting patches of the image and then treating them as a sequence analogous to how a natural language processing (NLP) model would treat words in a sentence. It utilizes the self-attention mechanism to capture dependencies between different patches and learn meaningful representations. By applying self-attention mechanisms, the ViT model can focus on relationships between patches, allowing it to understand the context and interactions between various parts of the image. This enables the model to effectively learn hierarchical features and spatial relationships within the image. Through this process, ViT can achieve competitive performance in image recognition tasks without using convolutional neural networks (CNNs), showcasing the effectiveness of the self-attention mechanism in capturing complex relationships in images.\n",
      "For details, please check the highlighted text on page(s): 9, 1, 2\n"
     ]
    }
   ],
   "source": [
    "paper='Vision Transformers'\n",
    "paper_path=\"ViT.pdf\"\n",
    "#create embeddings\n",
    "embedding=Embedder(engine='OpenAI')\n",
    "embedding.load_process('ViT.pdf')\n",
    "#set up vectorstore\n",
    "vectorstore=embedding.create_vectorstore(store_path=paper)\n",
    "# Fetch paper summary\n",
    "paper_summary= embedding.create_summary(llm_engine='OpenAI')\n",
    "# Instantiate journalist and author bot\n",
    "journalist=JournalistBot('OpenAI')\n",
    "author= AuthorBot('OpenAI',vectorstore)\n",
    "#Provide instruction\n",
    "journalist.instruct(topic='This paper introduces concepts of using transformers for images and computer vision tasks and how tradition transformers adapt to it',abstract=paper_summary)\n",
    "author.instruct('Paper introducing transformers and the attention mechanism,an architecture which can be widely used for language tasks')\n",
    "#Start conversation\n",
    "for i in range(4):\n",
    "    if i==0:\n",
    "        question=journalist.step('Start the conversation')\n",
    "        print(\"üë®‚Äçüè´ Journalist: \" + question)\n",
    "    else:\n",
    "        user=(input(\"Do you want to ask any question(y/n)\")).lower()\n",
    "        if user==\"y\":\n",
    "            question=input(\"Write your question\")\n",
    "            print(\"üë®‚Äçüè´ User: \" + question)\n",
    "        else:\n",
    "            question=journalist.step(answer)\n",
    "            print(\"üë®‚Äçüè´ Journalist: \" + question)\n",
    "    answer, source = author.step(question)\n",
    "    print(\"üë©‚Äçüéì Author: \" + answer)\n",
    "    phrases = [src.page_content for src in source]\n",
    "    highlight_PDF(paper_path, phrases, 'highlighted.pdf')\n",
    "    page_numbers = [str(src.metadata['page']+1) for src in source]\n",
    "    unique_page_numbers = list(set(page_numbers))\n",
    "    print((\"For details, please check the highlighted text on page(s): \" + ', '.join(unique_page_numbers)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7675f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
