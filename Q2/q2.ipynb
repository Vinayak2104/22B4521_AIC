{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cc3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import ArxivAPIWrapper\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import fitz\n",
    "import streamlit as st\n",
    "#from streamlit_chat import message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e967e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_API_KEY=\"\" #Enter your OpenAI API KEY\n",
    "class Embedder:\n",
    "    \"Embedding engine to create doc embeddings\"\n",
    "    def __init__(self,engine='OpenAI'):\n",
    "        \"\"\"\"Specify embedding model\n",
    "        Args:\n",
    "        -----------------\n",
    "        engine:the embedding model.\"\"\"\n",
    "        if engine=='OpenAI':\n",
    "            self.embeddings=OpenAIEmbeddings(api_key=OPEN_API_KEY)\n",
    "        else:\n",
    "            raise KeyError(\"Currently unsupported chat model type\")\n",
    "    def load_process(self,path):\n",
    "        \"\"\"Load and process PDF document.\n",
    "        Args:\n",
    "        ---------\n",
    "        path: path of the paper.\"\"\"\n",
    "        #load pdf\n",
    "        loader=PyMuPDFLoader(path)\n",
    "        documents=loader.load()\n",
    "        #process pdf\n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "        self.documents=text_splitter.split_documents(documents)\n",
    "    def create_vectorstore(self,store_path):\n",
    "        \"\"\"Create vector store for doc Q&A.\n",
    "        Args:\n",
    "        -------------------------\n",
    "        store_path:path of vector store.\n",
    "        Outputs:\n",
    "        vectorstore:the created vector store for holding embeddings\"\"\"\n",
    "        if not os.path.exists(store_path):\n",
    "            print('Embedding not found! Creating new ones')\n",
    "            self.vectorstore=FAISS.from_documents(self.documents,self.embeddings)\n",
    "            self.vectorstore.save_local(store_path)\n",
    "            \n",
    "        else:\n",
    "            print(\"Embeddings found! Loaded the computed ones\")\n",
    "            self.vectorstore=FAISS.load_local(store_path,self.embeddings,allow_dangerous_deserialization=True)\n",
    "        return self.vectorstore\n",
    "    def create_summary(self,llm_engine=None):\n",
    "        \"\"\"Create paper summary.\n",
    "        The summary is created by using LangChain's summarize_chain.\n",
    "        Args:\n",
    "        --------------\n",
    "        llm_engine: llm to use.\n",
    "        Outputs:\n",
    "        --------------\n",
    "        summary: the summary of the paper\"\"\"\n",
    "\n",
    "        if llm_engine is None:\n",
    "            raise KeyError('please specify a LLM engine to perform summarization')\n",
    "        elif llm_engine=='OpenAI':\n",
    "            llm=ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0.8,api_key=OPEN_API_KEY)\n",
    "        else:\n",
    "            raise KeyError('Currently unsupported chat model type')\n",
    "            \n",
    "        #Use LLM to summarize the paper\n",
    "        chain=load_summarize_chain(llm,chain_type='stuff')\n",
    "        summary=chain.run(self.documents[:20])\n",
    "        return summary\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot(ABC):\n",
    "    \"\"\"Class definition for a single chatbot with memory,created with LangChain. \"\"\"\n",
    "    def __init__(self,engine):\n",
    "        \"\"\"Initialize the LLM and its associated memory.\n",
    "        The memory can be an LangChain memory object,or a list of chat history.\n",
    "        Args:\n",
    "        --------------------\n",
    "        engine: llm to be used.\"\"\"\n",
    "        #Initialize llm\n",
    "        if engine=='OpenAI':\n",
    "            self.llm=ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0.8,api_key=OPEN_API_KEY)\n",
    "        else:\n",
    "            raise KeyError(\"Currently unsupported chat model type\")\n",
    "    @abstractmethod\n",
    "    def instruct(self):\n",
    "        \"\"\"Determine the context of chatbot interaction\"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        \"\"\"Action produced by the chatbot.\"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Prompt engineering for chatbot.\"\"\"\n",
    "        pass\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811d1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JournalistBot(Chatbot):\n",
    "    \"\"\"Class definition for the journalist bot,created with Langchain.\"\"\"\n",
    "    def __init__(self,engine):\n",
    "        \"\"\"Setup journalist bot.\n",
    "        Args:\n",
    "        -------------------------\n",
    "        engine: llm model to be used.\"\"\"\n",
    "        #Instantiate llm\n",
    "        super().__init__(engine)\n",
    "        #Instantiate memory\n",
    "        self.memory=ConversationBufferMemory(return_messages=True)\n",
    "    def instruct(self,topic,abstract):\n",
    "        \"\"\"Determine the context of journalist chatbot.\n",
    "        Args:\n",
    "        -----------\n",
    "        topic: the topics of the paper\n",
    "        abstract: the abstract of the paper.\"\"\"\n",
    "        self.topic=topic\n",
    "        self.abstract=abstract\n",
    "        #Define prompt template\n",
    "        prompt=ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"\"\"{input}\"\"\")\n",
    "            \n",
    "        ])\n",
    "        #Create conversation chain\n",
    "        self.conversation=ConversationChain(memory=self.memory,prompt=prompt,llm=self.llm,verbose=False)\n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Specify the behaviour of the journalist chatbot.\n",
    "        The prompt is generated and optimized with GPT-4.\n",
    "        outputs:\n",
    "        ----------------\n",
    "        prompts: instructions for chatbot.\"\"\"\n",
    "        prompt=f\"\"\"You are a technical journalist interested in {self.topic}, \n",
    "            Your task is to distill a recently published scientific paper on this topic through\n",
    "            an interview with the author, which is played by another chatbot.\n",
    "            Your objective is to ask comprehensive and technical questions \n",
    "            so that anyone who reads the interview can understand the paper's main ideas and contributions, \n",
    "            even without reading the paper itself. \n",
    "            You're provided with the paper's summary to guide your initial questions.\n",
    "            You must keep the following guidelines in mind:\n",
    "            - Focus exclusive on the technical content of the paper.\n",
    "            - Avoid general questions about {self.topic}, focusing instead on specifics related to the paper.\n",
    "            - Only ask one question at a time.\n",
    "            - Feel free to ask about the study's purpose, methods, results, and significance, \n",
    "            and clarify any technical terms or complex concepts. \n",
    "            - Your goal is to lead the conversation towards a clear and engaging summary.\n",
    "            - Do not include any prefixed labels like \"Interviewer:\" or \"Question:\" in your question.\n",
    "    \n",
    "            [Abstract]: {self.abstract}\"\"\"\n",
    "        return prompt\n",
    "    def step(self,prompt):\n",
    "        \"\"\"Journalist chatbot asks question\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "        prompt: Previous answer provided by the author bot.\"\"\"\n",
    "        response=self.conversation.predict(input=prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b91d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorBot(Chatbot):\n",
    "    \"\"\"Class definition for the author bot,created with Langchain.\"\"\"\n",
    "    def __init__(self,engine,vectorstore,debug=False):\n",
    "        \"\"\"Select the llm, as well as instantiate the memory for creating language chain in langchain.\n",
    "        \n",
    "        Args:\n",
    "        ----------\n",
    "        engine: llm model to be used.\n",
    "        vectorstore: embedding vectors of the pdf.\"\"\"\n",
    "        #Instantiate llm\n",
    "        super().__init__(engine)\n",
    "        #Intantiate memory\n",
    "        self.chat_history=[]\n",
    "        #Instantiate embedding index\n",
    "        self.vectorstore=vectorstore\n",
    "        self.debug=debug\n",
    "    def instruct(self,topic):\n",
    "        \"\"\"Determine the context of author chatbot.\n",
    "        Args:\n",
    "        -----------\n",
    "        topic: the topic of the paper.\"\"\"\n",
    "        #specify topic\n",
    "        self.topic=topic\n",
    "        #define prompt template\n",
    "        qa_prompt=ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "        ])\n",
    "        # Create conversation chain\n",
    "        self.conversation_qa=ConversationalRetrievalChain.from_llm(llm=self.llm,verbose=self.debug,retriever=self.vectorstore.as_retriever(search_kwargs={'k':5}),return_source_documents=True,combine_docs_chain_kwargs={'prompt':qa_prompt})\n",
    "    def step(self,prompt):\n",
    "        \"\"\"Author chatbot answers question.\n",
    "        Args:\n",
    "        ----------\n",
    "        prompt: question raised by journalist bot.\n",
    "        \n",
    "        Outputs:\n",
    "        ------------\n",
    "        answer: the author bot's answer\n",
    "        source_documents: documents that author bot used to answer questions\"\"\"\n",
    "        response=self.conversation_qa({'question':prompt,'chat_history':self.chat_history})\n",
    "        self.chat_history.append((prompt,response['answer']))\n",
    "        return response['answer'],response['source_documents']\n",
    "    def _specify_system_message(self):\n",
    "        \"\"\"Specify the behaviour of the author chatbot.\n",
    "        \n",
    "        outputs:\n",
    "        ------------\n",
    "        prompt: instructions for the chatbot.\"\"\"\n",
    "        prompt = f\"\"\"You are the author of a recently published scientific paper on {self.topic}.\n",
    "                You are being interviewed by a technical journalist who is played by another chatbot and\n",
    "                looking to write an article to summarize your paper.\n",
    "                Your task is to provide comprehensive, clear, and accurate answers to the journalist's questions.\n",
    "                Please keep the following guidelines in mind:\n",
    "                - Try to explain complex concepts and technical terms in an understandable way, without sacrificing accuracy.\n",
    "                - Your responses should primarily come from the relevant content of this paper, \n",
    "                  which will be provided to you in the following, but you can also use your broad knowledge in {self.topic} to \n",
    "                  provide context or clarify complex topics. \n",
    "                - Remember to differentiate when you are providing information directly from the paper versus \n",
    "                  when you're giving additional context or interpretation. Use phrases like 'According to the paper...' for direct information, \n",
    "                  and 'Based on general knowledge in the field...' when you're providing additional context.\n",
    "                - Only answer one question at a time. Ensure that each answer is complete before moving on to the next question.\n",
    "                - Do not include any prefixed labels like \"Author:\", \"Interviewee:\", Respond:\", or \"Answer:\" in your answer.\n",
    "    \"\"\"\n",
    "    \n",
    "        prompt += \"\"\"Given the following context, please answer the question.\n",
    "    \n",
    "    {context}\"\"\"\n",
    "    \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e585d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_PDF(file_path,phrases,output_path):\n",
    "    \"\"\"Search and highlight given texts in PDF.\n",
    "    \n",
    "    Args:\n",
    "    ----------------\n",
    "    file_path: PDF file path\n",
    "    phrases: a list of texts(in strings)\n",
    "    output_path: save and output PDF\"\"\"\n",
    "    #Open PDF\n",
    "    doc=fitz.open(file_path)\n",
    "    #Search doc\n",
    "    for page in doc:\n",
    "        for phrase in phrases:\n",
    "            text_instances=page.search_for(phrase)\n",
    "            #Highlight\n",
    "            for inst in text_instances:\n",
    "                highlight=page.add_highlight_annot(inst)\n",
    "    doc.save(output_path,garbage=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55742596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings found! Loaded the computed ones\n",
      "üë®‚Äçüè´ Journalist: What motivated you to explore the use of transformers, commonly used in natural language processing, for image recognition tasks in your paper?\n",
      "üë©‚Äçüéì Author: In our paper, we were motivated to explore the use of transformers for image recognition tasks due to the success of transformers in natural language processing (NLP). Transformers have become the standard in NLP tasks because of their computational efficiency and scalability, allowing for training models with unprecedented sizes and achieving state-of-the-art results. We wanted to investigate whether transformers could also excel in computer vision tasks, specifically image recognition, without relying on convolutional neural networks (CNNs), which are traditionally used in this domain. By applying transformers directly to sequences of image patches, we aimed to demonstrate that transformers could perform well in image classification tasks and potentially outperform CNNs while requiring fewer computational resources.\n",
      "For details, please check the highlighted text on page(s): 9, 2, 1\n",
      "üë®‚Äçüè´ Journalist: Can you explain how the Vision Transformer (ViT) model directly applies transformers to sequences of image patches, and how this approach differs from traditional convolutional neural networks (CNNs) in the context of image recognition tasks?\n",
      "üë©‚Äçüéì Author: The Vision Transformer (ViT) model applies transformers to sequences of image patches by first reshaping the input image into a sequence of flattened 2D patches. Each patch is then flattened and mapped to a constant latent vector size, which serves as the input sequence for the Transformer. This approach allows the ViT model to process images using the same mechanisms as transformers used in natural language processing tasks.\n",
      "\n",
      "When compared to traditional Convolutional Neural Networks (CNNs) in image recognition tasks, the ViT model has shown promising results. By pre-training on large datasets and transferring the knowledge to smaller datasets, ViT has achieved excellent performance on various image classification benchmarks like ImageNet, CIFAR-100, and VTAB. The ViT model has been able to match or exceed the state-of-the-art results for image classification while requiring fewer computational resources compared to CNNs. This demonstrates that transformers, when applied directly to sequences of image patches, can be a competitive alternative to CNNs for image recognition tasks.\n",
      "For details, please check the highlighted text on page(s): 3, 9, 2, 1\n",
      "üë®‚Äçüè´ Journalist: How does the ViT model demonstrate the importance of scale in training, especially when pre-trained on large datasets and transferred to smaller tasks in the context of image recognition?\n",
      "üë©‚Äçüéì Author: The ViT (Vision Transformer) model demonstrates the importance of scale in training for image recognition tasks by showing that when pre-trained on large datasets, such as ImageNet-21k or JFT-300M, it can achieve excellent results. Specifically, when pre-trained on these larger datasets and then transferred to tasks with fewer data points, the ViT model approaches or surpasses the state-of-the-art performance on multiple image recognition benchmarks. This showcases that large-scale training can outweigh the traditional biases in models like Convolutional Neural Networks (CNNs) and lead to superior generalization capabilities, especially when applied to downstream tasks with limited data.\n",
      "For details, please check the highlighted text on page(s): 4, 2, 5, 6, 7\n",
      "üë®‚Äçüè´ Journalist: How does the ViT model exhibit less image-specific bias compared to Convolutional Neural Networks (CNNs) in the context of image recognition tasks, as mentioned in your paper's findings?\n",
      "üë©‚Äçüéì Author: According to the paper, the ViT model exhibits less image-specific bias compared to Convolutional Neural Networks (CNNs) in image recognition tasks due to its ability to generalize well even when trained on larger datasets. When trained on larger datasets like ImageNet-21k or JFT-300M, ViT shows excellent performance and approaches or beats the state of the art on multiple image recognition benchmarks. This indicates that large-scale training trumps inductive bias, allowing ViT to achieve high accuracy levels on tasks with fewer data points. Additionally, ViT's performance on challenging datasets like ImageNet, CIFAR-100, and the VTAB suite is further improved by models like ViT-H/14, showcasing its effectiveness in reducing image-specific bias.\n",
      "For details, please check the highlighted text on page(s): 4, 2, 5, 6, 7\n"
     ]
    }
   ],
   "source": [
    "paper='Vision Transformers'\n",
    "paper_path=\"ViT.pdf\"\n",
    "#create embeddings\n",
    "embedding=Embedder(engine='OpenAI')\n",
    "embedding.load_process('ViT.pdf')\n",
    "#set up vectorstore\n",
    "vectorstore=embedding.create_vectorstore(store_path=paper)\n",
    "# Fetch paper summary\n",
    "paper_summary= embedding.create_summary(llm_engine='OpenAI')\n",
    "# Instantiate journalist and author bot\n",
    "journalist=JournalistBot('OpenAI')\n",
    "author= AuthorBot('OpenAI',vectorstore)\n",
    "#Provide instruction\n",
    "journalist.instruct(topic='This paper introduces concepts of using transformers for images and computer vision tasks and how tradition transformers adapt to it',abstract=paper_summary)\n",
    "author.instruct('Paper introducing transformers and the attention mechanism,an architecture which can be widely used for language tasks')\n",
    "#Start conversation\n",
    "for i in range(4):\n",
    "    if i==0:\n",
    "        question=journalist.step('Start the conversation')\n",
    "    else:\n",
    "        question=journalist.step(answer)\n",
    "    print(\"üë®‚Äçüè´ Journalist: \" + question)\n",
    "    answer, source = author.step(question)\n",
    "    print(\"üë©‚Äçüéì Author: \" + answer)\n",
    "    phrases = [src.page_content for src in source]\n",
    "    highlight_PDF(paper_path, phrases, 'highlighted.pdf')\n",
    "    page_numbers = [str(src.metadata['page']+1) for src in source]\n",
    "    unique_page_numbers = list(set(page_numbers))\n",
    "    print((\"For details, please check the highlighted text on page(s): \" + ', '.join(unique_page_numbers)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7675f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
